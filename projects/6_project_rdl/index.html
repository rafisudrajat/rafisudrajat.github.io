<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Scaling Long-Horizon Online POMDP Planning with Rapid State Space Sampling | Robust Decision Making Lab </title> <meta name="author" content="You R. Name"> <meta name="description" content="Analytically solving the reference-based POMDP bellman backup removes the need of neumerical optimisation during planning, and enables seemless integration of state space sampling and belief space sampling to solve long horizon POMDPs online."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rafisudrajat.github.io/projects/6_project_rdl/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <img src="/assets/img/rdl_logo.png?11e5b2abd7dfd8659b915ab84202c2be" class="img-fluid" style="height: 60px;" alt=""> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">software </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">contact </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Scaling long-horizon online pomdp planning with rapid state space sampling</h1> </header> <article> <h4>Yuanchu Liang, Edward Kim, Wil Thomason, Zachary Kingston, Hanna Kurniawati and Lydia E. Kavraki</h4> <hr> <p class="text-justify"> Many realistic robotic problems contain high levels of uncertainties, long horizon planning and information gatherings are often required to find a robust action to execute. Despite tremendous improvement in the scalability of POMDP solvers, long-horizon pomdps (e.g., $\geq$15 steps) remain difficult to solve. This paper proposes a new approximate online POMDP solver, called <strong>Reference-Based Online POMDP Planning via Rapid State Space Sampling (ROP-RaS3)</strong>. ROP-RaS3 uses novel extremely fast sampling-based motion planning techniques to sample the state space and generate a diverse set of macro actions online which are then used to bias belief-space sampling and infer high-quality policies <strong>without</strong> requiring exhaustive enumeration of the action space -- a fundamental constraint for modern online POMDP solvers. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/ropras/tree_compare-480.webp 480w,/assets/img/project_img/ropras/tree_compare-800.webp 800w,/assets/img/project_img/ropras/tree_compare-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/ropras/tree_compare.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="tree comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> A illustrated comparison of the belief tree constructed via exhuastive backup (left) and reference backup (right). The reference based belief tree is sparser and deeper thanks to the removal of exhuastive enumeration during construction. </div> <p class="text-justify"> The reference-based POMDP incorporates a reference policy $\bar{\pi}$ such that the objective of the POMDP is to find the best reward gathering action without deviating too far from the reference. $$V(b) = \sup_{\pi\in\Pi}\left[R(b,\pi), \frac{1}{\eta}\text{KL}(\pi\| \bar{\pi}) + \gamma\sum_{a,o}P(o\mid a, b)\pi(a\mid b)V(\tau(b,a,o))\right]$$ Here $b$ is the current belief, $R$ is the stochastic reward, $\eta$ is a temperature term, $KL$ denotes the KL divergence, $P(o\mid a, b)$ denotes the conditional probability of receiving an observation based on the action and the belief, and $\tau$ denotes the belief update function. It turns out that such objective can be analytically solved to give, $$ V(b) = \frac{1}{\eta}\log \left[\sum_a \bar{\pi}(a\mid b)\exp\left\{\eta\left[R(b,a)+\gamma\sum_o P(o\mid a, b)V(\tau(b,a,o))\right]\right\}\right] $$ Notice that this form is an expectation of the future discounted reward under the reference policy and it can be estimated via monte-carlo sampling. To further boost computatiional efficiency in solving long horizon robotic problems, we sample deterministic motion plans from <a href="https://github.com/KavrakiLab/vamp" rel="external nofollow noopener" target="_blank">Vector Accelerated Motion Planning (VAMP)</a> as macro actions in our belief tree constructions. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/ropras/experiment_pics-480.webp 480w,/assets/img/project_img/ropras/experiment_pics-800.webp 800w,/assets/img/project_img/ropras/experiment_pics-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/ropras/experiment_pics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="experiment pictures" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Benchmark environments with obstacles (grey), landmarks (purple), danger zones (red), starting locations (orange), goals and targets (green). <br> </div> <div class="text-justify"> We benchmark our method against other state-of-the-art methods across four different domains and show that our methods performs exceeds other methods by a large extent. The environments are showing above and detailed below, <br> <strong>Light Dark</strong>: The classical POMDP problem, the agent initiallised randomly is required to localised within the purple light and then navigate to the goal. <br> <strong>Maze2D</strong>: A very long horizon problem. The agent that could start at two different entires is required utilise scattered lights to collect state information and navigate to the goal. <br> <strong>Random3D</strong>: A environment with randomised obstacles to create arbitrary narrow passages. This environment is made to stress test the robustness of the proposed method if underlying reference policies fails probabilistically. <br> <strong>Multi-Drone Tag</strong>: The agent needs to control four drones to capture the target. Target information is provided if any drone is within a certain detection radius. The drone can teleport to opposite sides if it reaches one end of the map, but drones cannot. <br> The methods we compar against include, <br> <strong> B-VAMP </strong>: The underlying reference policy with belief update during execution loops. No POMDP planning on top of the reference. <br> <strong> Ref-Basic </strong>: The original reference-based solver proposed from our lab in the earlier work. <br> <strong> POMCP </strong>: The classical state-of-the-art POMDP online planner. <br> <strong> R-POMCP </strong>: POMCP but with actions sampled from our reference policy. <br> <strong> MAGIC </strong>: An integrated learning and planning framework that generate learnt macro actions for planning. <br> <strong> RMAG </strong>: The improved version of MAGIC that uses special modularised memory neural networks to generate better macro actions. <br> Across all domains, ROPRaS3 has clear advantages in terms of successfully completing the problem while being efficient in terms of number of steps being executed in each task. In Maze2D, the agent equipped with ROPRaS3 figures out to use nearby lights to localise even though it might take an extra few steps. In Random3D, ROPRas3's performance do not drop much as the reference policy failure rates increases. And in Multi-Drone Tag, ROPRaS3 succesfully spread the drone out to capture the target from all directions, preventing it from escaping, demonstrating long horizon searching and planning capabilities. Details experimental results are given below. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/ropras/ropras_tab1-480.webp 480w,/assets/img/project_img/ropras/ropras_tab1-800.webp 800w,/assets/img/project_img/ropras/ropras_tab1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/ropras/ropras_tab1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="experiment pictures" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/ropras/ropras_tab2-480.webp 480w,/assets/img/project_img/ropras/ropras_tab2-800.webp 800w,/assets/img/project_img/ropras/ropras_tab2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/ropras/ropras_tab2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="experiment pictures" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure class="text-center"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/ropras/ropras_tab3-480.webp 480w,/assets/img/project_img/ropras/ropras_tab3-800.webp 800w,/assets/img/project_img/ropras/ropras_tab3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/ropras/ropras_tab3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 50%; " title="experiment pictures" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2> References </h2> <div class="publications"> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Robust Decision Making Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@tabler/icons-webfont@latest/tabler-icons.min.css"> </html>