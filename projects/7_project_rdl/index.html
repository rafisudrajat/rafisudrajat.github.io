<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Approximate POMDP Solving without Numerical Optimisation (NeurIPS + IJCAI) | Robust Decision Making Lab </title> <meta name="author" content="You R. Name"> <meta name="description" content="POMDP solver approximation"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rafisudrajat.github.io/projects/7_project_rdl/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <img src="/assets/img/rdl_logo.png?11e5b2abd7dfd8659b915ab84202c2be" class="img-fluid" style="height: 60px;" alt=""> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/software/">software </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">contact </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Approximate pomdp solving without numerical optimisation (neurips + ijcai)</h1> </header> <article> <p class="text-justify"> Current state-of-the-art online POMDP planners rely on algorithms that compute estimates of the expected total reward of performing an exhaustive set of actions before optimising over these estimates. As such, these solvers *exhaustively enumerate* over the entire action space at each node in the belief tree, which massively hinders fast computation of a close-to-optimal solution for problems with large action spaces and long horizons. This problem is even worse when the environment is also dynamically changing at each execution step. </p> <p class="text-justify"> The core difficulty is the curse of history where the set of possible futures branches by the size of the action space and grows exponentially with respect to the horizon. Most existing methods try to abstract the problem into a simpler one by either reducing the size of the action space or relying on *macro actions*---i.e. a set of open-loop action sequences---to reduce the planning horizon. Still, the fundamental problem---i.e. exhaustive action enumeration---remains. </p> <p class="text-justify"> This project seeks to ameliorate this undesirable requirement by employing *reference-based* methods. In short, this is a form of Kullback-Leibler (KL) regularisation on the objective relative to some *reference policy* which encodes some (though not all) information about the solution. We propose two main contributions to this end. </p> <ul> <li>Reference-Based POMDPs <a target="_blank" href="/assets/pdf/papers/neurips23.pdf">here</a> </li> <li>Partialy Observable Reference Policy Programming &lt;/ul&gt; <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/analytical/deep-480.webp 480w,/assets/img/project_img/analytical/deep-800.webp 800w,/assets/img/project_img/analytical/deep-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/analytical/deep.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="deep sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Current state-of-the-art online planners are "exhaustive" in the sense that they branch over actions at each node before proceeding deeper down the tree, so meaningful long horizon futures are seldom explored. In contrast, reference-based approaches sample using information encoded by the reference policy and can sample deeper and sparser while gradually improving the policy. </div> <h2> Reference-Based POMDPs </h2> <p class="text-justify"> The notion of a Reference-Based POMDP (RBPOMDP) is a reformulation of a POMDP whose objective is penalised by the KL-divergence between a chosen and nominal reference policy. The form of ojbective allows analytical action optimisation at each belief node, so that the value can be approximated by estimating expectations under the reference policy. This property accommodates solvers that have been shown to perform effectively on certain long-horizon tasks and forms a first step in our analysis. The full paper is available <a target="_blank" href="/assets/pdf/papers/neurips23.pdf">here</a>&lt;/li&gt;. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/analytical/nav3d-480.webp 480w,/assets/img/project_img/analytical/nav3d-800.webp 800w,/assets/img/project_img/analytical/nav3d-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/analytical/nav3d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="policy trace" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The executed policy trace of a reference-based approximate planner (in blue) whose reference policy naively takes actions which minimise the Euclidean distance. The deformation achieved by the new policy is clearly seen. </div> <h2> Partially Observable Reference Policy Programming </h2> <p class="text-justify"> While approximate RBPOMDP solvers have been shown to perform well with well-chosen reference policies, the formulation comes at the cost that the solution has a baked-in commitment to the reference policy. In general, it is unclear a priori which reference policies would yield near optimal policies for the original POMDP of interest, and the computed solution is vulnerable to reference policy mis-specification. The aim of Partially Observable Reference Policy Programming (PORPP) is to exploit the information encoded the the reference policy to sample towards promising belief nodes, while simultaneous enforcing a gradual and systematic update to the reference policy. In doing so, one can guarantee asymptotic convergence to the solution of the original POMDP and also adaptive policies to changing environments or models. Furthermore, our theoretical analysis shows that the performance loss of the exact scheme is bounded by the *average* of the sampling errors, meaning the algorithm is less sensitive to large approximation errors. The full paper is available <a target="_blank" href="/assets/pdf/papers/ijcai25.pdf">here</a>&lt;/li&gt;. </p> <p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/analytical/deform1-480.webp 480w,/assets/img/project_img/analytical/deform1-800.webp 800w,/assets/img/project_img/analytical/deform1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/analytical/deform1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example scenario for POMDP evaluation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_img/analytical/deform2-480.webp 480w,/assets/img/project_img/analytical/deform2-800.webp 800w,/assets/img/project_img/analytical/deform2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_img/analytical/deform2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example scenario for POMDP evaluation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </p> <div class="caption"> Executed trace of a helicopter during a search and rescue mission in the Corsica region. The pilot needs to avoid a danger zone which appears without warning during the middle of the run. PORPP can adapt quickly to the change, executing a well-crafted policy. </div> <h2> References </h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim25:PORPP" class="col-sm-8"> <div class="title">Partially Observable Reference Policy Programming</div> <div class="author"> E. Kim and H. Kurniawati </div> <div class="periodical"> <em>In Proc. Int. Joint Conference on Artificial Intelligence (IJCAI)</em>, 2025 </div> <div class="periodical"> To appear </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/papers/ijcai25.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kim25:PORPP</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, E. and Kurniawati, H.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Partially Observable Reference Policy Programming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. Int. Joint Conference on Artificial Intelligence (IJCAI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{To appear}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim23:RBPOMDP" class="col-sm-8"> <div class="title">Reference-Based POMDPs</div> <div class="author"> M. Hoerger, H. Kurniawati, and A. Elfes </div> <div class="periodical"> <em>International Journal of Robotics Research (IJRR)</em>, 2022 </div> <div class="periodical"> To appear </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/papers/neurips23.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Kim23:RBPOMDP</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hoerger, M. and Kurniawati, H. and Elfes, A.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reference-Based {POMDPs}}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Robotics Research (IJRR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{To appear}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <h2> People </h2> <ul> <li>Edward Kim</li> <li>Hanna Kurniawati</li> <li>Yohan Karunanayake</li> </ul> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Robust Decision Making Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@tabler/icons-webfont@latest/tabler-icons.min.css"> </html>